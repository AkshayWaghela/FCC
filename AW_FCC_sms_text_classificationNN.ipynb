{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshayWaghela/FCC/blob/notebooks/AW_FCC_sms_text_classificationNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#?pd.read_table"
      ],
      "metadata": {
        "id": "Hv3f_Z5Rdvwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "d1 = pd.read_table(train_file_path,names=['C','T'])\n",
        "d1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt1 = pd.read_table(test_file_path,names=['C','T'])\n",
        "dt1.head()"
      ],
      "metadata": {
        "id": "2dQhzZx-f9JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for basic info: rows, columns, data types, missing values\n",
        "df = d1\n",
        "df.info()\n",
        "\n",
        "# Check for any missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check basic statistics of the data\n",
        "df.describe(include='all')\n",
        "\n",
        "# View the distribution of categories in 'C' column\n",
        "print(df['C'].value_counts())\n"
      ],
      "metadata": {
        "id": "DHahTweNg1yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "22XgaL6Lf69J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "d1.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = d1.drop_duplicates(ignore_index=True)\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ULs-8Q61jfZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "v3izc3o0jfLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.C.value_counts()"
      ],
      "metadata": {
        "id": "j8-_EHGNje6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "zfqX9dzgjegl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.C.value_counts(normalize=True).plot(kind='bar')"
      ],
      "metadata": {
        "id": "Axr4blZ6FCa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_set = []\n",
        "label_set = []\n",
        "for i in range(0,df.shape[0],1):\n",
        "    sent_set.append(df['T'][i])\n",
        "    label_set.append(df['C'][i])"
      ],
      "metadata": {
        "id": "7SWeICTuFCGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GISmr18oHijf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.to_excel('SMS_DATA.xlsx')"
      ],
      "metadata": {
        "id": "gD0axlKsHcvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage I. Preliminary actions. Preparing of needed sets\\n\")\n",
        "\n",
        "sentences_new_set = []\n",
        "labels_new_set = []\n",
        "for i in range(0, d1.shape[0], 1):\n",
        "    sentences_new_set.append(d1['T'][i])\n",
        "    labels_new_set.append(d1['C'][i])"
      ],
      "metadata": {
        "id": "CSe7n38ZTbJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_vocabulary = 1000    # You can choose the size of vocabulary yourself in the range [500 .. 1500] (it should be divisible by 500)\n",
        "embedding_dimension = 64  # You can choose the size of dimention yourself in the range [32 .. 256] (it should be divisible by 32)\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "threshold = 0.5           # You can choose the size of threshold yourself in the range [0 .. 1]\n",
        "oov_token = \"<OOV>\"\n",
        "test_size, valid_size = 0.05, 0.2\n",
        "num_epochs = 20           # You can choose the number of epochs yourself in the range [20 .. 50] (it should be divisible by 5)\n",
        "drop_level = 0.3          # You can choose the size of drop level yourself in the range [0 .. 1]"
      ],
      "metadata": {
        "id": "yHW_QRmPT9oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(d1.shape[0] * (1 - test_size - valid_size))\n",
        "valid_bound = int(d1.shape[0] * (1 - valid_size))\n",
        "\n",
        "train_sentences = sentences_new_set[0 : train_size]\n",
        "valid_sentences = sentences_new_set[train_size : valid_bound]\n",
        "test_sentences = sentences_new_set[valid_bound : ]\n",
        "\n",
        "train_labels_str = labels_new_set[0 : train_size]\n",
        "valid_labels_str = labels_new_set[train_size : valid_bound]\n",
        "test_labels_str = labels_new_set[valid_bound : ]"
      ],
      "metadata": {
        "id": "xSPuNCvCTrgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage II. Labels transformations\\n\")\n",
        "\n",
        "train_labels = [0] * len(train_labels_str)\n",
        "for ind, item in enumerate(train_labels_str):\n",
        "    if item == 'ham':\n",
        "        train_labels[ind] = 1\n",
        "    else:\n",
        "        train_labels[ind] = 0\n",
        "\n",
        "valid_labels = [0] * len(valid_labels_str)\n",
        "for ind, item in enumerate(valid_labels_str):\n",
        "    if item == 'ham':\n",
        "        valid_labels[ind] = 1\n",
        "    else:\n",
        "        valid_labels[ind] = 0\n",
        "\n",
        "test_labels = [0] * len(test_labels_str)\n",
        "for ind, item in enumerate(test_labels_str):\n",
        "    if item == 'ham':\n",
        "        test_labels[ind] = 1\n",
        "    else:\n",
        "        test_labels[ind] = 0\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "valid_labels = np.array(valid_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "#print(test_labels)"
      ],
      "metadata": {
        "id": "YpAaY0wsUEpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, collections, pickle, os # nltk - Natural Language Toolkit\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# %matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15, 5)\n",
        "plt.style.use('ggplot')\n",
        "seed = 42\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action = \"ignore\")\n",
        "warnings.simplefilter(action = 'ignore', category = Warning)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qHJelJiZoDn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage III. Tokenization\\n\")\n",
        "\n",
        "tokenizer = Tokenizer(num_words = size_vocabulary,\n",
        "                      oov_token = oov_token,\n",
        "                      lower = False)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "LyPxYcu-naR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "size_voc = len(word_index) + 1\n",
        "max_len = max([len(i) for i in train_sequences])\n",
        "train_set = pad_sequences(train_sequences,\n",
        "                                padding = padding_type,\n",
        "                                maxlen = max_len,\n",
        "                                truncating = trunc_type)\n",
        "\n",
        "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
        "valid_set = pad_sequences(valid_sequences,\n",
        "                               padding = padding_type,\n",
        "                               maxlen = max_len,\n",
        "                               truncating = trunc_type)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_set = pad_sequences(test_sequences,\n",
        "                               padding = padding_type,\n",
        "                               maxlen = max_len,\n",
        "                               truncating = trunc_type)"
      ],
      "metadata": {
        "id": "0n7kGyJ7oJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage IV. Model building\\n\")\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(size_voc, embedding_dimension, input_length = max_len),\n",
        "    Bidirectional(LSTM(100)),\n",
        "    Dropout(drop_level),\n",
        "    Dense(20, activation = 'relu'),\n",
        "    Dropout(drop_level),\n",
        "    Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "4ETyBiOEovk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage V. Model compiling & fitting\\n\")\n",
        "optim = Adam(learning_rate = 0.001)\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = optim,\n",
        "              metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LXY3NRyipRca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?model.fit()"
      ],
      "metadata": {
        "id": "AJIiYn4PMu5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "id": "TMDux_lOPb-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_set,\n",
        "                    train_labels,\n",
        "                    epochs = 5,validation_data = (valid_set, valid_labels),\n",
        "                    verbose = 1)"
      ],
      "metadata": {
        "id": "r0Hq-veppmng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' in s]\n",
        "\n",
        "    plt.figure(figsize = (12, 5), dpi = 100)\n",
        "    COLOR = 'gray'\n",
        "\n",
        "    plt.rc('legend', fontsize = 14)   # legend fontsize\n",
        "    plt.rc('figure', titlesize = 12)  # fontsize of the figure title\n",
        "\n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return\n",
        "\n",
        "    ## As loss always exists\n",
        "    epochs = range(1, len(history.history[loss_list[0]]) + 1)\n",
        "\n",
        "    ## Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.subplots_adjust(wspace = 2, hspace = 2)\n",
        "    plt.rcParams['text.color'] = 'black'\n",
        "    plt.rcParams['axes.titlecolor'] = 'black'\n",
        "    plt.rcParams['axes.labelcolor'] = COLOR\n",
        "    plt.rcParams['xtick.color'] = COLOR\n",
        "    plt.rcParams['ytick.color'] = COLOR\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b-o',\n",
        "                 label = 'Train (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g',\n",
        "                 label = 'Valid (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n",
        "\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(facecolor = 'gray', loc = 'best')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    ## Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.subplots_adjust(wspace = 2, hspace = 2)\n",
        "    plt.rcParams['text.color'] = 'black'\n",
        "    plt.rcParams['axes.titlecolor'] = 'black'\n",
        "    plt.rcParams['axes.labelcolor'] = COLOR\n",
        "    plt.rcParams['xtick.color'] = COLOR\n",
        "    plt.rcParams['ytick.color'] = COLOR\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b-o',\n",
        "                 label = 'Train (' + str(format(history.history[l][-1],'.4f'))+')')\n",
        "    for l in val_acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'g',\n",
        "                 label = 'Valid (' + str(format(history.history[l][-1],'.4f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(facecolor = 'gray', loc = 'best')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_conf_matr(conf_matr, classes,\n",
        "                          normalize = False,\n",
        "                          title = 'Confusion matrix',\n",
        "                          cmap = plt.cm.winter):\n",
        "  \"\"\"\n",
        "  Citation\n",
        "  ---------\n",
        "  http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "  \"\"\"\n",
        "  import itertools\n",
        "\n",
        "  accuracy = np.trace(conf_matr) / np.sum(conf_matr).astype('float')\n",
        "  sns.set(font_scale = 1.4)\n",
        "\n",
        "  plt.figure(figsize = (12, 8))\n",
        "  plt.imshow(conf_matr, interpolation = 'nearest', cmap = cmap)\n",
        "  title = '\\n' + title + '\\n'\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "\n",
        "  if classes is not None:\n",
        "      tick_marks = np.arange(len(classes))\n",
        "      plt.xticks(tick_marks, classes, rotation = 45)\n",
        "      plt.yticks(tick_marks, classes)\n",
        "\n",
        "\n",
        "\n",
        "  thresh = conf_matr.max() / 1.5 if normalize else conf_matr.max() / 2\n",
        "  for i, j in itertools.product(range(conf_matr.shape[0]), range(conf_matr.shape[1])):\n",
        "      if normalize:\n",
        "          plt.text(j, i, \"{:0.2f}%\".format(conf_matr[i, j] * 100),\n",
        "                    horizontalalignment = \"center\",\n",
        "                    fontweight = 'bold',\n",
        "                    color = \"white\" if conf_matr[i, j] > thresh else \"black\")\n",
        "      else:\n",
        "          plt.text(j, i, \"{:,}\".format(conf_matr[i, j]),\n",
        "                    horizontalalignment = \"center\",\n",
        "                    fontweight = 'bold',\n",
        "                    color = \"white\" if conf_matr[i, j] > thresh else \"black\")\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label\\n\\nAccuracy = {:0.2f}%; Error = {:0.2f}%'.format(accuracy * 100, (1 - accuracy) * 100))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_words(set, number):\n",
        "  words_counter = collections.Counter([word for sentence in set for word in sentence.split()]) # finding words along with count\n",
        "  most_counted = words_counter.most_common(number)\n",
        "  most_count = pd.DataFrame(most_counted, columns = [\"Words\", \"Amount\"]).sort_values(by = \"Amount\") # sorted data frame\n",
        "  most_count.plot.barh(x = \"Words\",\n",
        "                       y = \"Amount\",\n",
        "                       color = \"blue\",\n",
        "                       figsize = (10, 15))\n",
        "  for i, v in enumerate(most_count[\"Amount\"]):\n",
        "    plt.text(v, i,\n",
        "             \" \" + str(v),\n",
        "             color = 'black',\n",
        "             va = 'center',\n",
        "             fontweight = 'bold')\n",
        "\n",
        "def word_cloud(tag):\n",
        "  df_words_nl = ' '.join(list(df_spam[df_spam['feature'] == tag]['message']))\n",
        "  df_wc_nl = WordCloud(width = 600, height = 512).generate(df_words_nl)\n",
        "  plt.figure(figsize = (13, 9), facecolor = 'k')\n",
        "  plt.imshow(df_wc_nl)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout(pad = 1)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EF-MjGjyP1KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stage VI. Results visualization\\n\")\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "ajNGkbaDPqGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_score = model.evaluate(test_set, test_labels, batch_size = embedding_dimension, verbose = 1)\n",
        "print(f\"Test accuracy: {model_score[1] * 100:0.2f}% \\t\\t Test error: {model_score[0]:0.4f}\")"
      ],
      "metadata": {
        "id": "kimScuWfQASQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bLSTM = model.predict(test_set)\n",
        "\n",
        "y_prediction = [0] * y_pred_bLSTM.shape[0]\n",
        "for ind, item in enumerate(y_pred_bLSTM):\n",
        "    if item > threshold:\n",
        "        y_prediction[ind] = 1\n",
        "    else:\n",
        "        y_prediction[ind] = 0\n",
        "\n",
        "conf_m_bLSTM = confusion_matrix(test_labels, y_prediction)\n",
        "class_rep_bLSTM = classification_report(test_labels, y_prediction)\n",
        "print('\\t\\t\\tClassification report:\\n\\n', class_rep_bLSTM, '\\n')\n",
        "plot_conf_matr(conf_m_bLSTM, classes = ['Spam','Ham'], normalize = False, title = 'Confusion matrix for bLSTM')"
      ],
      "metadata": {
        "id": "XLz6rxiEQSTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can change this message (as any short sentence) yourself\n",
        "message_example = [\"Darling, please give me a cup of tea\"]\n",
        "\n",
        "message_example_tp = pad_sequences(tokenizer.texts_to_sequences(message_example),\n",
        "                                   maxlen = max_len,\n",
        "                                   padding = padding_type,\n",
        "                                   truncating = trunc_type)\n",
        "\n",
        "pred = float(model.predict(message_example_tp))\n",
        "if (pred > threshold):\n",
        "    print (\"This message is a real text\")\n",
        "else:\n",
        "    print(\"This message is a spam message\")"
      ],
      "metadata": {
        "id": "5WF76IAGQuZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "# function to predict messages based on modelva\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "\n",
        "  # You can change this message (as any short sentence) yourself\n",
        "  message_example = [pred_text]\n",
        "\n",
        "  message_example_tp = pad_sequences(tokenizer.texts_to_sequences(message_example),\n",
        "                                   maxlen = max_len,\n",
        "                                   padding = padding_type,\n",
        "                                   truncating = trunc_type)\n",
        "\n",
        "  pred = float(model.predict(message_example_tp))\n",
        "  prediction = ''\n",
        "  if (pred > threshold):\n",
        "      print (\"ham\")\n",
        "      prediction = 'ham'\n",
        "  else:\n",
        "      prediction = 'spam'\n",
        "      print(\"spam\")\n",
        "\n",
        "  return ([pred,prediction])\n",
        "\n",
        "pred_text = \"how are you doing today?\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    print(prediction,\"---\",ans)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qg-dhM_yky-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T45MSOB7kzIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=df['C'], palette='coolwarm')\n",
        "plt.title(\"Distribution of Spam vs. Ham Messages\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_N9DP0uokzPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ra1RCcDSk9TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['length'] = df['T'].apply(len)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df[df['C']=='ham']['length'], bins=50, color='blue', label='Ham', kde=True)\n",
        "sns.histplot(df[df['C']=='spam']['length'], bins=50, color='red', label='Spam', kde=True)\n",
        "plt.legend()\n",
        "plt.title(\"Message Length Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BGgTQxZJk9Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "deycm6oik_i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1K9i63x9lEW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "spam_words = ' '.join(df[df['C']=='spam']['T'])\n",
        "ham_words = ' '.join(df[df['C']=='ham']['T'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color=\"black\").generate(spam_words))\n",
        "plt.title(\"Spam Word Cloud\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color=\"black\").generate(ham_words))\n",
        "plt.title(\"Ham Word Cloud\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hgx6pwz9lEuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qieuUoIKlGDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5ifIH1GlLds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "words = ' '.join(df['T']).split()\n",
        "word_freq = Counter(words)\n",
        "common_words = pd.DataFrame(word_freq.most_common(10), columns=['Word', 'Count'])\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x='Count', y='Word', data=common_words, palette='viridis')\n",
        "plt.title(\"Top 10 Most Common Words\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IdUw_HjwlLxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Message Length (Spam):\", df[df['C']=='spam']['length'].mean())\n",
        "print(\"Average Message Length (Ham):\", df[df['C']=='ham']['length'].mean())\n"
      ],
      "metadata": {
        "id": "t1m9paAAlMP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toBT8iyVlSD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(test_labels, y_prediction)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Spam','Ham'], yticklabels=['Spam','Ham'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W2ooXeBAlTKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uh1Nfs48lVN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(test_labels, y_prediction, target_names=['Spam','Ham']))\n"
      ],
      "metadata": {
        "id": "8ZichGIdlV1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNoz-tpNlaGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_indices = np.where(test_labels != y_prediction)[0]\n",
        "\n",
        "for idx in misclassified_indices[:5]:  # Show first 5 misclassified messages\n",
        "    print(f\"Actual: {'Ham' if test_labels[idx] == 1 else 'Spam'}, Predicted: {'Ham' if y_prediction[idx] == 1 else 'Spam'}\")\n",
        "    print(f\"Message: {test_sentences[idx]}\\n\")\n"
      ],
      "metadata": {
        "id": "sweWrktflb--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ENCN2u1hlfzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wQ0Xyzll_Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Calculate lengths of messages\n",
        "df['length'] = df['T'].apply(len)\n",
        "\n",
        "# Extract spam & ham lengths\n",
        "spam_lengths = df[df['C'] == 'spam']['length']\n",
        "ham_lengths = df[df['C'] == 'ham']['length']\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"Spam Mean Length: {np.mean(spam_lengths):.2f}, Std Dev: {np.std(spam_lengths):.2f}\")\n",
        "print(f\"Ham Mean Length: {np.mean(ham_lengths):.2f}, Std Dev: {np.std(ham_lengths):.2f}\")\n"
      ],
      "metadata": {
        "id": "xq-UizBPl_f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8okO2hPl_-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "levene_test = stats.levene(spam_lengths, ham_lengths)\n",
        "print(f\"Levene's Test p-value: {levene_test.pvalue:.5f}\")\n",
        "\n",
        "if levene_test.pvalue < 0.05:\n",
        "    print(\"Variance is significantly different.\")\n",
        "else:\n",
        "    print(\"Variances are similar.\")\n"
      ],
      "metadata": {
        "id": "LQyAQUCymCap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_test = stats.ttest_ind(spam_lengths, ham_lengths, equal_var=False)  # Welch’s t-test\n",
        "print(f\"T-test Statistic: {t_test.statistic:.2f}, p-value: {t_test.pvalue:.5f}\")\n",
        "\n",
        "if t_test.pvalue < 0.05:\n",
        "    print(\"There is a significant difference in message length between spam and ham.\")\n",
        "else:\n",
        "    print(\"No significant difference in length.\")\n"
      ],
      "metadata": {
        "id": "nRsOJRCEmFxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cohens_d(x, y):\n",
        "    return (np.mean(x) - np.mean(y)) / np.sqrt((np.std(x) ** 2 + np.std(y) ** 2) / 2)\n",
        "\n",
        "effect_size = cohens_d(spam_lengths, ham_lengths)\n",
        "print(f\"Cohen's d: {effect_size:.2f}\")\n",
        "\n",
        "if effect_size >= 0.8:\n",
        "    print(\"Large difference in message lengths between spam and ham.\")\n",
        "elif effect_size >= 0.5:\n",
        "    print(\"Moderate difference in message lengths.\")\n",
        "else:\n",
        "    print(\"Small difference in message lengths.\")\n"
      ],
      "metadata": {
        "id": "pkN5G2OBmIez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "labels = (df['C'] == 'spam').astype(int)  # Convert 'spam' = 1, 'ham' = 0\n",
        "auc = roc_auc_score(labels, df['length'])\n",
        "print(f\"AUC Score for Length-Based Classification: {auc:.3f}\")\n",
        "\n",
        "if auc > 0.8:\n",
        "    print(\"Message length is a strong predictor of spam.\")\n",
        "elif auc > 0.6:\n",
        "    print(\"Message length provides some predictive power.\")\n",
        "else:\n",
        "    print(\"Message length alone is not a reliable predictor of spam.\")\n"
      ],
      "metadata": {
        "id": "0yUiyn5bmMRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# KDE Plot\n",
        "sns.kdeplot(spam_lengths, label=\"Spam\", fill=True, color=\"red\", alpha=0.5)\n",
        "sns.kdeplot(ham_lengths, label=\"Ham\", fill=True, color=\"blue\", alpha=0.5)\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Distribution of Message Lengths\", fontsize=14)\n",
        "plt.xlabel(\"Message Length\", fontsize=12)\n",
        "plt.ylabel(\"Density\", fontsize=12)\n",
        "plt.legend()\n",
        "\n",
        "# Show KDE plot\n",
        "plt.show()\n",
        "\n",
        "# Boxplot for comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=df['C'], y=df['length'], palette={\"spam\": \"red\", \"ham\": \"blue\"})\n",
        "\n",
        "# Annotate Cohen’s d\n",
        "plt.text(0.5, max(df['length']) * 0.95, f\"Cohen's d = {effect_size:.2f}\", ha='center', fontsize=12)\n",
        "\n",
        "plt.title(\"Boxplot of Message Lengths\")\n",
        "plt.xlabel(\"Message Category\")\n",
        "plt.ylabel(\"Message Length\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "06eMc62NmPUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QXQZZRGOmt2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-S8YMm0nLGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Compute message lengths\n",
        "df['length'] = df['T'].apply(len)\n",
        "\n",
        "# Separate spam and ham lengths\n",
        "spam_lengths = df[df['C'] == 'spam']['length']\n",
        "ham_lengths = df[df['C'] == 'ham']['length']\n",
        "\n",
        "# Fit normal distributions\n",
        "spam_mean, spam_std = np.mean(spam_lengths), np.std(spam_lengths)\n",
        "ham_mean, ham_std = np.mean(ham_lengths), np.std(ham_lengths)\n",
        "\n",
        "# Prior probabilities\n",
        "P_spam = len(spam_lengths) / len(df)\n",
        "P_ham = len(ham_lengths) / len(df)\n",
        "\n",
        "# Define function to compute P(Spam | Length)\n",
        "def spam_likelihood(length):\n",
        "    P_length_given_spam = norm.pdf(length, spam_mean, spam_std)\n",
        "    P_length_given_ham = norm.pdf(length, ham_mean, ham_std)\n",
        "\n",
        "    P_length = P_length_given_spam * P_spam + P_length_given_ham * P_ham\n",
        "\n",
        "    return (P_length_given_spam * P_spam) / P_length\n",
        "\n",
        "# Example: Likelihood for a message of 100 characters\n",
        "length_example = 100\n",
        "spam_prob = spam_likelihood(length_example) * 100\n",
        "\n",
        "print(f\"Likelihood of a message being spam given length {length_example}: {spam_prob:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ie_01wTxnLVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_likelihood(130)*100"
      ],
      "metadata": {
        "id": "-pTMTt5SnLrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenize words\n",
        "spam_words = \" \".join(df[df['C'] == 'spam']['T']).split()\n",
        "ham_words = \" \".join(df[df['C'] == 'ham']['T']).split()\n",
        "\n",
        "# Word frequencies\n",
        "spam_word_freq = Counter(spam_words)\n",
        "ham_word_freq = Counter(ham_words)\n",
        "\n",
        "# Compute likelihood of spam for a given word\n",
        "def spam_word_likelihood(word):\n",
        "    P_word_given_spam = spam_word_freq[word] / sum(spam_word_freq.values())\n",
        "    P_word_given_ham = ham_word_freq[word] / sum(ham_word_freq.values())\n",
        "\n",
        "    P_word = P_word_given_spam * P_spam + P_word_given_ham * P_ham\n",
        "\n",
        "    return (P_word_given_spam * P_spam) / P_word if P_word > 0 else 0\n",
        "\n",
        "# Example: Compute likelihood for the word \"free\"\n",
        "word_example = \"free\"\n",
        "word_spam_prob = spam_word_likelihood(word_example) * 100\n",
        "\n",
        "print(f\"Likelihood of a message being spam given it contains '{word_example}': {word_spam_prob:.2f}%\")\n"
      ],
      "metadata": {
        "id": "P8dNnaNfndGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSfkjNGIndh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNPvOvO_pfFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from collections import Counter\n",
        "\n",
        "# Compute message lengths\n",
        "df['length'] = df['T'].apply(len)\n",
        "\n",
        "# Separate spam and ham lengths\n",
        "spam_lengths = df[df['C'] == 'spam']['length']\n",
        "ham_lengths = df[df['C'] == 'ham']['length']\n",
        "\n",
        "# Fit normal distributions\n",
        "spam_mean, spam_std = np.mean(spam_lengths), np.std(spam_lengths)\n",
        "ham_mean, ham_std = np.mean(ham_lengths), np.std(ham_lengths)\n",
        "\n",
        "# Prior probabilities\n",
        "P_spam = len(spam_lengths) / len(df)\n",
        "P_ham = len(ham_lengths) / len(df)\n",
        "\n",
        "# Compute spam probability for different lengths\n",
        "length_range = np.arange(10, 300, 5)\n",
        "spam_probs = [norm.pdf(l, spam_mean, spam_std) * P_spam /\n",
        "              (norm.pdf(l, spam_mean, spam_std) * P_spam + norm.pdf(l, ham_mean, ham_std) * P_ham)\n",
        "              for l in length_range]\n",
        "\n",
        "# Plot spam probability vs. length\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.lineplot(x=length_range, y=spam_probs, color='red')\n",
        "plt.xlabel(\"Message Length\")\n",
        "plt.ylabel(\"P(Spam | Length)\")\n",
        "plt.title(\"Spam Probability Based on Message Length\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ---- Word-Based Probability ----\n",
        "spam_words = \" \".join(df[df['C'] == 'spam']['T']).split()\n",
        "ham_words = \" \".join(df[df['C'] == 'ham']['T']).split()\n",
        "\n",
        "spam_word_freq = Counter(spam_words)\n",
        "ham_word_freq = Counter(ham_words)\n",
        "\n",
        "# Compute likelihood for common words\n",
        "common_words = [\"free\", \"win\", \"offer\", \"urgent\", \"call\", \"meeting\", \"hello\"]\n",
        "word_spam_probs = [(w, spam_word_freq[w] / sum(spam_word_freq.values()) * P_spam /\n",
        "                    ((spam_word_freq[w] / sum(spam_word_freq.values())) * P_spam +\n",
        "                     (ham_word_freq[w] / sum(ham_word_freq.values())) * P_ham))\n",
        "                   if w in spam_word_freq else (w, 0) for w in common_words]\n",
        "\n",
        "# Convert to dictionary for plotting\n",
        "word_dict = {w: p for w, p in word_spam_probs}\n",
        "\n",
        "# Plot spam probability for words\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=list(word_dict.keys()), y=list(word_dict.values()), color='blue')\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"P(Spam | Word)\")\n",
        "plt.title(\"Spam Probability Based on Word Usage\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a2U018snpfLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQuOL0hopgJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMuRGJkzrC16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}